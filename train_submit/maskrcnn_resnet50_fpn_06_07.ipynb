{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/itsuki9180/hubmap-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import sys\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sys.path.append(\"/kaggle/input/detection-wheel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs, masks, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.imgs = imgs\n",
    "        self.masks = masks\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.imgs[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        obj_ids = np.unique(mask)\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        masks = [np.where(mask== obj_ids[i, None, None],1,0) for i in range(len(obj_ids))]\n",
    "        masks = np.array(masks)\n",
    "\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.nonzero(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        try:\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            #print(area,area.shape,area.dtype)\n",
    "        except:\n",
    "            area = torch.tensor([[0],[0]])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        #print(masks.shape)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models import list_models\n",
    "detection_models = list_models(module=torchvision.models.detection)\n",
    "detection_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    #model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=\"DEFAULT\", weights_backbone=ResNet50_Weights.IMAGENET1K_V2)\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\", weights_backbone=ResNet50_Weights.IMAGENET1K_V2) \n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features # 1024\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels # 256\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "n_imgs = len(glob.glob('/kaggle/input/hubmap-making-dataset/train/image/*'))\n",
    "print(\"Total Number of Images\" ,n_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_verification = False\n",
    "if pipeline_verification :\n",
    "    n_imgs = 10\n",
    "    EPOCHS = 2\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(range(n_imgs))):\n",
    "    if i!=0: \n",
    "        continue\n",
    "    all_imgs = sorted(glob.glob('/kaggle/input/hubmap-making-dataset/train/image/*.png'))\n",
    "    all_masks = sorted(glob.glob('/kaggle/input/hubmap-making-dataset/train/mask/*.png'))\n",
    "    if pipeline_verification :\n",
    "        all_imgs = np.array(all_imgs[:10])\n",
    "        all_masks = np.array(all_masks[:10])\n",
    "    else :\n",
    "        all_imgs = np.array(all_imgs)\n",
    "        all_masks = np.array(all_masks)\n",
    "    train_img = all_imgs[train_index]\n",
    "    train_mask = all_masks[train_index]\n",
    "    val_img = all_imgs[test_index]\n",
    "    val_mask = all_masks[test_index]\n",
    "    dataset_train = PennFudanDataset(train_img, train_mask, get_transform(train=True))\n",
    "    dataset_val = PennFudanDataset(val_img, val_mask, get_transform(train=False))\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=4, shuffle=True, num_workers=os.cpu_count(), pin_memory=True, drop_last=True, collate_fn=utils.collate_fn)\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False, num_workers=os.cpu_count(), pin_memory=True,collate_fn=utils.collate_fn)\n",
    "    \n",
    "    model = get_model_instance_segmentation(num_classes=2)\n",
    "    model.to(device)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=0.0003, weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_one_epoch(model, optimizer, train_dl, device, epoch, print_freq=50)\n",
    "        evaluate(model, val_dl, device=device)\n",
    "        scheduler.step()\n",
    "        model_path = f'fold_{i}_epoch{epoch}.pth'\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import sys\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import cv2\n",
    "from skimage.morphology import binary_dilation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sys.path.append(\"/kaggle/input/detection-wheel\")\n",
    "###################################################################################\n",
    "\n",
    "# Install pycocotools package\n",
    "import os\n",
    "!mkdir /kaggle/working/packages\n",
    "!cp -r /kaggle/input/pycocotools/* /kaggle/working/packages\n",
    "os.chdir(\"/kaggle/working/packages/pycocotools-2.0.6/\")\n",
    "!python setup.py install -q\n",
    "!pip install . --no-index --find-links /kaggle/working/packages/ -q\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "import base64\n",
    "import numpy as np\n",
    "from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import zlib\n",
    "\n",
    "def encode_binary_mask(mask: np.ndarray) -> t.Text:\n",
    "    \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n",
    "\n",
    "    # check input mask --\n",
    "    if mask.dtype != np.bool:\n",
    "        raise ValueError(\n",
    "            \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n",
    "            mask.dtype)\n",
    "\n",
    "    mask = np.squeeze(mask)\n",
    "    if len(mask.shape) != 2:\n",
    "        raise ValueError(\n",
    "            \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n",
    "            mask.shape)\n",
    "\n",
    "    # convert input mask to expected COCO API input --\n",
    "    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "    mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "    mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "    # RLE encode mask --\n",
    "    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "    # compress and base64 encoding --\n",
    "    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "    base64_str = base64.b64encode(binary_str)\n",
    "    return base64_str\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs, transforms):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = imgs\n",
    "        self.name_indices = [os.path.splitext(os.path.basename(i))[0] for i in imgs]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = self.imgs[idx]\n",
    "        name = self.name_indices[idx]\n",
    "        array = tiff.imread(img_path)\n",
    "        img = Image.fromarray(array)\n",
    "        \n",
    "        img, _ = self.transforms(img, img)\n",
    "\n",
    "        return img, name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "#############################################################################################\n",
    "import torchvision\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=None, weights_backbone=None)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "###########################################################\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    return T.Compose(transforms)\n",
    "################################################################\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model_instance_segmentation(num_classes=2)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('/kaggle/input/hubmap-well-tuned/well_tuned_weight.pth'))\n",
    "model.eval()\n",
    "print()\n",
    "\n",
    "all_imgs = glob.glob('/kaggle/input/hubmap-hacking-the-human-vasculature/test/*.tif')\n",
    "dataset_test = PennFudanDataset(all_imgs, get_transform(train=False))\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n",
    "ids = []\n",
    "heights = []\n",
    "widths = []\n",
    "prediction_strings = []\n",
    "sample = None\n",
    "with torch.no_grad():\n",
    "    for img, idx in test_dl:\n",
    "        img = img.to(device)\n",
    "        pred = model(img)\n",
    "        if sample is None: sample=pred\n",
    "        pred_string = ''\n",
    "        for m in range(len(pred[0]['masks'])):\n",
    "            mask = pred[0]['masks'][m].detach().permute(1,2,0).cpu().numpy()\n",
    "            mask = np.where(mask>0.5, 1, 0).astype(np.bool)\n",
    "            mask = binary_dilation(mask)\n",
    "            \n",
    "            score = pred[0]['scores'][m].detach().cpu().numpy()\n",
    "            encoded = encode_binary_mask(mask)\n",
    "            if m==0:\n",
    "                pred_string += f\"0 {score} {encoded.decode('utf-8')}\"\n",
    "\n",
    "            else:\n",
    "                pred_string += f\" 0 {score} {encoded.decode('utf-8')}\"\n",
    "        b, c, h, w = img.shape\n",
    "        ids.append(idx[0])\n",
    "        heights.append(h)\n",
    "        widths.append(w)\n",
    "        prediction_strings.append(pred_string)\n",
    "#######################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = tiff.imread('/kaggle/input/hubmap-hacking-the-human-vasculature/test/72e40acccadf.tif')\n",
    "plt.imshow(array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_imgs)==1:\n",
    "    top20 = [sample[0]['masks'][i].cpu().numpy().reshape(512, 512) for i in range(min(20,len(sample[0]['masks'])))]\n",
    "    \n",
    "    pred_img = np.zeros((512,512), dtype=np.float32)\n",
    "    for i, j in enumerate(top20):\n",
    "        pred_img += j * (1 - 1/len(top20)*i)\n",
    "        pred_img = np.clip(pred_img, 0, 1)\n",
    "        print(sample[0]['scores'][i].cpu().numpy())\n",
    "        plt.imshow(j)\n",
    "        plt.show()\n",
    "        \n",
    "    plt.imshow(pred_img)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
