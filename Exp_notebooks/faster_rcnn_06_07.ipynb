{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is just to understand pytorch implementation of faster RCNN, \n",
    "Code taken From \n",
    "from torchvision.models.detection.faster_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "# from ._utils import overwrite_eps\n",
    "# from ..._internally_replaced_utils import load_state_dict_from_url\n",
    "\n",
    "# from .anchor_utils import AnchorGenerator\n",
    "# from .generalized_rcnn import GeneralizedRCNN\n",
    "# from .rpn import RPNHead, RegionProposalNetwork\n",
    "# from .roi_heads import RoIHeads\n",
    "# from .transform import GeneralizedRCNNTransform\n",
    "# from .backbone_utils import resnet_fpn_backbone, _validate_trainable_layers, mobilenet_backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"FasterRCNN\", \"fasterrcnn_resnet50_fpn\", \"fasterrcnn_mobilenet_v3_large_320_fpn\",\n",
    "    \"fasterrcnn_mobilenet_v3_large_fpn\"\n",
    "]\n",
    "\n",
    "\n",
    "class FasterRCNN(GeneralizedRCNN):\n",
    "    \"\"\"\n",
    "    Implements Faster R-CNN.\n",
    "\n",
    "    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
    "    image, and should be in 0-1 range. Different images can have different sizes.\n",
    "\n",
    "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
    "\n",
    "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
    "    containing:\n",
    "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
    "\n",
    "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
    "    losses for both the RPN and the R-CNN.\n",
    "\n",
    "    During inference, the model requires only the input tensors, and returns the post-processed\n",
    "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
    "    follows:\n",
    "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
    "        - scores (Tensor[N]): the scores or each prediction\n",
    "\n",
    "    Args:\n",
    "        backbone (nn.Module): the network used to compute the features for the model.\n",
    "            It should contain a out_channels attribute, which indicates the number of output\n",
    "            channels that each feature map has (and it should be the same for all feature maps).\n",
    "            The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
    "        num_classes (int): number of output classes of the model (including the background).\n",
    "            If box_predictor is specified, num_classes should be None.\n",
    "        min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
    "        max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
    "        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
    "            They are generally the mean values of the dataset on which the backbone has been trained\n",
    "            on\n",
    "        image_std (Tuple[float, float, float]): std values used for input normalization.\n",
    "            They are generally the std values of the dataset on which the backbone has been trained on\n",
    "        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
    "            maps.\n",
    "        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
    "        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
    "        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
    "        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
    "        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
    "        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
    "        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
    "            considered as positive during training of the RPN.\n",
    "        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
    "            considered as negative during training of the RPN.\n",
    "        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
    "            for computing the loss\n",
    "        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
    "            of the RPN\n",
    "        rpn_score_thresh (float): during inference, only return proposals with a classification score\n",
    "            greater than rpn_score_thresh\n",
    "        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
    "            the locations indicated by the bounding boxes\n",
    "        box_head (nn.Module): module that takes the cropped feature maps as input\n",
    "        box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
    "            classification logits and box regression deltas.\n",
    "        box_score_thresh (float): during inference, only return proposals with a classification score\n",
    "            greater than box_score_thresh\n",
    "        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
    "        box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
    "        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
    "            considered as positive during training of the classification head\n",
    "        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
    "            considered as negative during training of the classification head\n",
    "        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
    "            classification head\n",
    "        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
    "            of the classification head\n",
    "        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
    "            bounding boxes\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> import torch\n",
    "        >>> import torchvision\n",
    "        >>> from torchvision.models.detection import FasterRCNN\n",
    "        >>> from torchvision.models.detection.rpn import AnchorGenerator\n",
    "        >>> # load a pre-trained model for classification and return\n",
    "        >>> # only the features\n",
    "        >>> backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "        >>> # FasterRCNN needs to know the number of\n",
    "        >>> # output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "        >>> # so we need to add it here\n",
    "        >>> backbone.out_channels = 1280\n",
    "        >>>\n",
    "        >>> # let's make the RPN generate 5 x 3 anchors per spatial\n",
    "        >>> # location, with 5 different sizes and 3 different aspect\n",
    "        >>> # ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "        >>> # map could potentially have different sizes and\n",
    "        >>> # aspect ratios\n",
    "        >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "        >>>                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "        >>>\n",
    "        >>> # let's define what are the feature maps that we will\n",
    "        >>> # use to perform the region of interest cropping, as well as\n",
    "        >>> # the size of the crop after rescaling.\n",
    "        >>> # if your backbone returns a Tensor, featmap_names is expected to\n",
    "        >>> # be ['0']. More generally, the backbone should return an\n",
    "        >>> # OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "        >>> # feature maps to use.\n",
    "        >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "        >>>                                                 output_size=7,\n",
    "        >>>                                                 sampling_ratio=2)\n",
    "        >>>\n",
    "        >>> # put the pieces together inside a FasterRCNN model\n",
    "        >>> model = FasterRCNN(backbone,\n",
    "        >>>                    num_classes=2,\n",
    "        >>>                    rpn_anchor_generator=anchor_generator,\n",
    "        >>>                    box_roi_pool=roi_pooler)\n",
    "        >>> model.eval()\n",
    "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "        >>> predictions = model(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, num_classes=None,\n",
    "                 # transform parameters\n",
    "                 min_size=800, max_size=1333,\n",
    "                 image_mean=None, image_std=None,\n",
    "                 # RPN parameters\n",
    "                 rpn_anchor_generator=None, rpn_head=None,\n",
    "                 rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,\n",
    "                 rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,\n",
    "                 rpn_nms_thresh=0.7,\n",
    "                 rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n",
    "                 rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
    "                 rpn_score_thresh=0.0,\n",
    "                 # Box parameters\n",
    "                 box_roi_pool=None, box_head=None, box_predictor=None,\n",
    "                 box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,\n",
    "                 box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
    "                 box_batch_size_per_image=512, box_positive_fraction=0.25,\n",
    "                 bbox_reg_weights=None):\n",
    "\n",
    "        if not hasattr(backbone, \"out_channels\"):\n",
    "            raise ValueError(\n",
    "                \"backbone should contain an attribute out_channels \"\n",
    "                \"specifying the number of output channels (assumed to be the \"\n",
    "                \"same for all the levels)\")\n",
    "\n",
    "        assert isinstance(rpn_anchor_generator, (AnchorGenerator, type(None)))\n",
    "        assert isinstance(box_roi_pool, (MultiScaleRoIAlign, type(None)))\n",
    "\n",
    "        if num_classes is not None:\n",
    "            if box_predictor is not None:\n",
    "                raise ValueError(\"num_classes should be None when box_predictor is specified\")\n",
    "        else:\n",
    "            if box_predictor is None:\n",
    "                raise ValueError(\"num_classes should not be None when box_predictor \"\n",
    "                                 \"is not specified\")\n",
    "\n",
    "        out_channels = backbone.out_channels\n",
    "\n",
    "        if rpn_anchor_generator is None:\n",
    "            anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "            aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "            rpn_anchor_generator = AnchorGenerator(\n",
    "                anchor_sizes, aspect_ratios\n",
    "            )\n",
    "        if rpn_head is None:\n",
    "            rpn_head = RPNHead(\n",
    "                out_channels, rpn_anchor_generator.num_anchors_per_location()[0]\n",
    "            )\n",
    "\n",
    "        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n",
    "        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n",
    "\n",
    "        rpn = RegionProposalNetwork(\n",
    "            rpn_anchor_generator, rpn_head,\n",
    "            rpn_fg_iou_thresh, rpn_bg_iou_thresh,\n",
    "            rpn_batch_size_per_image, rpn_positive_fraction,\n",
    "            rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh,\n",
    "            score_thresh=rpn_score_thresh)\n",
    "\n",
    "        if box_roi_pool is None:\n",
    "            box_roi_pool = MultiScaleRoIAlign(\n",
    "                featmap_names=['0', '1', '2', '3'],\n",
    "                output_size=7,\n",
    "                sampling_ratio=2)\n",
    "\n",
    "        if box_head is None:\n",
    "            resolution = box_roi_pool.output_size[0]\n",
    "            representation_size = 1024\n",
    "            box_head = TwoMLPHead(\n",
    "                out_channels * resolution ** 2,\n",
    "                representation_size)\n",
    "\n",
    "        if box_predictor is None:\n",
    "            representation_size = 1024\n",
    "            box_predictor = FastRCNNPredictor(\n",
    "                representation_size,\n",
    "                num_classes)\n",
    "\n",
    "        roi_heads = RoIHeads(\n",
    "            # Box\n",
    "            box_roi_pool, box_head, box_predictor,\n",
    "            box_fg_iou_thresh, box_bg_iou_thresh,\n",
    "            box_batch_size_per_image, box_positive_fraction,\n",
    "            bbox_reg_weights,\n",
    "            box_score_thresh, box_nms_thresh, box_detections_per_img)\n",
    "\n",
    "        if image_mean is None:\n",
    "            image_mean = [0.485, 0.456, 0.406]\n",
    "        if image_std is None:\n",
    "            image_std = [0.229, 0.224, 0.225]\n",
    "        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)\n",
    "\n",
    "        super(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)\n",
    "\n",
    "\n",
    "class TwoMLPHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard heads for FPN-based models\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        representation_size (int): size of the intermediate representation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, representation_size):\n",
    "        super(TwoMLPHead, self).__init__()\n",
    "\n",
    "        self.fc6 = nn.Linear(in_channels, representation_size)\n",
    "        self.fc7 = nn.Linear(representation_size, representation_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FastRCNNPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(FastRCNNPredictor, self).__init__()\n",
    "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
    "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'fasterrcnn_resnet50_fpn_coco':\n",
    "        'https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth',\n",
    "    'fasterrcnn_mobilenet_v3_large_320_fpn_coco':\n",
    "        'https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_320_fpn-907ea3f9.pth',\n",
    "    'fasterrcnn_mobilenet_v3_large_fpn_coco':\n",
    "        'https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth'\n",
    "}\n",
    "\n",
    "\n",
    "def fasterrcnn_resnet50_fpn(pretrained=False, progress=True,\n",
    "                            num_classes=91, pretrained_backbone=True, trainable_backbone_layers=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.\n",
    "\n",
    "    Reference: `\"Faster R-CNN: Towards Real-Time Object Detection with\n",
    "    Region Proposal Networks\" <https://arxiv.org/abs/1506.01497>`_.\n",
    "\n",
    "    The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each\n",
    "    image, and should be in ``0-1`` range. Different images can have different sizes.\n",
    "\n",
    "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
    "\n",
    "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
    "    containing:\n",
    "\n",
    "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (``Int64Tensor[N]``): the class label for each ground-truth box\n",
    "\n",
    "    The model returns a ``Dict[Tensor]`` during training, containing the classification and regression\n",
    "    losses for both the RPN and the R-CNN.\n",
    "\n",
    "    During inference, the model requires only the input tensors, and returns the post-processed\n",
    "    predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as\n",
    "    follows, where ``N`` is the number of detections:\n",
    "\n",
    "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
    "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
    "        - labels (``Int64Tensor[N]``): the predicted labels for each detection\n",
    "        - scores (``Tensor[N]``): the scores of each detection\n",
    "\n",
    "    For more details on the output, you may refer to :ref:`instance_seg_output`.\n",
    "\n",
    "    Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        >>> # For training\n",
    "        >>> images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "        >>> labels = torch.randint(1, 91, (4, 11))\n",
    "        >>> images = list(image for image in images)\n",
    "        >>> targets = []\n",
    "        >>> for i in range(len(images)):\n",
    "        >>>     d = {}\n",
    "        >>>     d['boxes'] = boxes[i]\n",
    "        >>>     d['labels'] = labels[i]\n",
    "        >>>     targets.append(d)\n",
    "        >>> output = model(images, targets)\n",
    "        >>> # For inference\n",
    "        >>> model.eval()\n",
    "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "        >>> predictions = model(x)\n",
    "        >>>\n",
    "        >>> # optionally, if you want to export the model to ONNX:\n",
    "        >>> torch.onnx.export(model, x, \"faster_rcnn.onnx\", opset_version = 11)\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on COCO train2017\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        num_classes (int): number of output classes of the model (including the background)\n",
    "        pretrained_backbone (bool): If True, returns a model with backbone pre-trained on Imagenet\n",
    "        trainable_backbone_layers (int): number of trainable (not frozen) resnet layers starting from final block.\n",
    "            Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.\n",
    "    \"\"\"\n",
    "    trainable_backbone_layers = _validate_trainable_layers(\n",
    "        pretrained or pretrained_backbone, trainable_backbone_layers, 5, 3)\n",
    "\n",
    "    if pretrained:\n",
    "        # no need to download the backbone if pretrained is set\n",
    "        pretrained_backbone = False\n",
    "    backbone = resnet_fpn_backbone('resnet50', pretrained_backbone, trainable_layers=trainable_backbone_layers)\n",
    "    model = FasterRCNN(backbone, num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['fasterrcnn_resnet50_fpn_coco'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "        overwrite_eps(model, 0.0)\n",
    "    return model\n",
    "\n",
    "\n",
    "def _fasterrcnn_mobilenet_v3_large_fpn(weights_name, pretrained=False, progress=True, num_classes=91,\n",
    "                                       pretrained_backbone=True, trainable_backbone_layers=None, **kwargs):\n",
    "    trainable_backbone_layers = _validate_trainable_layers(\n",
    "        pretrained or pretrained_backbone, trainable_backbone_layers, 6, 3)\n",
    "\n",
    "    if pretrained:\n",
    "        pretrained_backbone = False\n",
    "    backbone = mobilenet_backbone(\"mobilenet_v3_large\", pretrained_backbone, True,\n",
    "                                  trainable_layers=trainable_backbone_layers)\n",
    "\n",
    "    anchor_sizes = ((32, 64, 128, 256, 512, ), ) * 3\n",
    "    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "\n",
    "    model = FasterRCNN(backbone, num_classes, rpn_anchor_generator=AnchorGenerator(anchor_sizes, aspect_ratios),\n",
    "                       **kwargs)\n",
    "    if pretrained:\n",
    "        if model_urls.get(weights_name, None) is None:\n",
    "            raise ValueError(\"No checkpoint is available for model {}\".format(weights_name))\n",
    "        state_dict = load_state_dict_from_url(model_urls[weights_name], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True,\n",
    "                                          trainable_backbone_layers=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a low resolution Faster R-CNN model with a MobileNetV3-Large FPN backbone tunned for mobile use-cases.\n",
    "    It works similarly to Faster R-CNN with ResNet-50 FPN backbone. See\n",
    "    :func:`~torchvision.models.detection.fasterrcnn_resnet50_fpn` for more\n",
    "    details.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
    "        >>> model.eval()\n",
    "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "        >>> predictions = model(x)\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on COCO train2017\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        num_classes (int): number of output classes of the model (including the background)\n",
    "        pretrained_backbone (bool): If True, returns a model with backbone pre-trained on Imagenet\n",
    "        trainable_backbone_layers (int): number of trainable (not frozen) resnet layers starting from final block.\n",
    "            Valid values are between 0 and 6, with 6 meaning all backbone layers are trainable.\n",
    "    \"\"\"\n",
    "    weights_name = \"fasterrcnn_mobilenet_v3_large_320_fpn_coco\"\n",
    "    defaults = {\n",
    "        \"min_size\": 320,\n",
    "        \"max_size\": 640,\n",
    "        \"rpn_pre_nms_top_n_test\": 150,\n",
    "        \"rpn_post_nms_top_n_test\": 150,\n",
    "        \"rpn_score_thresh\": 0.05,\n",
    "    }\n",
    "\n",
    "    kwargs = {**defaults, **kwargs}\n",
    "    return _fasterrcnn_mobilenet_v3_large_fpn(weights_name, pretrained=pretrained, progress=progress,\n",
    "                                              num_classes=num_classes, pretrained_backbone=pretrained_backbone,\n",
    "                                              trainable_backbone_layers=trainable_backbone_layers, **kwargs)\n",
    "\n",
    "\n",
    "def fasterrcnn_mobilenet_v3_large_fpn(pretrained=False, progress=True, num_classes=91, pretrained_backbone=True,\n",
    "                                      trainable_backbone_layers=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a high resolution Faster R-CNN model with a MobileNetV3-Large FPN backbone.\n",
    "    It works similarly to Faster R-CNN with ResNet-50 FPN backbone. See\n",
    "    :func:`~torchvision.models.detection.fasterrcnn_resnet50_fpn` for more\n",
    "    details.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "        >>> model.eval()\n",
    "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "        >>> predictions = model(x)\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on COCO train2017\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "        num_classes (int): number of output classes of the model (including the background)\n",
    "        pretrained_backbone (bool): If True, returns a model with backbone pre-trained on Imagenet\n",
    "        trainable_backbone_layers (int): number of trainable (not frozen) resnet layers starting from final block.\n",
    "            Valid values are between 0 and 6, with 6 meaning all backbone layers are trainable.\n",
    "    \"\"\"\n",
    "    weights_name = \"fasterrcnn_mobilenet_v3_large_fpn_coco\"\n",
    "    defaults = {\n",
    "        \"rpn_score_thresh\": 0.05,\n",
    "    }\n",
    "\n",
    "    kwargs = {**defaults, **kwargs}\n",
    "    return _fasterrcnn_mobilenet_v3_large_fpn(weights_name, pretrained=pretrained, progress=progress,\n",
    "                                              num_classes=num_classes, pretrained_backbone=pretrained_backbone,\n",
    "                                              trainable_backbone_layers=trainable_backbone_layers, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
